{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPctdE2VxZCQ/sEU4/j+GPC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nxthxnael/Machine-Learning-Essentials/blob/master/Practical%20Session%201/Nathanael_Mutua_SC212_0588_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification Algorithms**"
      ],
      "metadata": {
        "id": "O5vhpK6Y2DGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1**\n",
        "\n",
        "I will import the Iris dataset from the scikit learn library.\n",
        "\n",
        "> *It contains features for different iris flower species, which the model will learn to categorize.*"
      ],
      "metadata": {
        "id": "SM3ati5g3leB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading & creating a dataframe for the Iris Dataset**\n",
        "\n",
        "- I will run `sklearn.datasets import load_iris`\n",
        "- I will also import pandas, load the dataset and create a dataframe\n",
        "\n",
        "Then I will display the first few rows of the dataset."
      ],
      "metadata": {
        "id": "dPisvGUZ15nz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY8RBpkDxibu"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# loading the dataset\n",
        "iris = load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a dataframe\n",
        "\n",
        "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "iris_df['target'] = iris.target\n",
        "\n",
        "# I'm adding the target column to include the additional information\n",
        "# (numerical values representing the species)\n",
        "\n",
        "print(iris_df.head())"
      ],
      "metadata": {
        "id": "7Jet-0976Wvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n"
      ],
      "metadata": {
        "id": "BNaYrz6dCI_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2**\n",
        "Carrying out the classification algorithms using 5 algorithms.\n",
        "\n",
        "Where for each, you must follow a standard pipeline: **Train the model, make predictions, display the confusion matrix, and compute performance metrics**."
      ],
      "metadata": {
        "id": "qGMOH09vAd8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. K Nearest Neighbour (KNN)**\n",
        "\n",
        "**Got me wondering what exactlly is KNN?**\n",
        "KNN is a **\"lazy learner\"** that does not build a formal model during training; instead, it stores the data and makes decisions during the prediction phase based on the classes of the closest neighbors.\n",
        "\n",
        "Key considerationsI will have: Use the **\"Rule of Thumb\"** ($K = \\sqrt{n}$) or **Cross-Validation** to select the best value for $K$.\n",
        "\n",
        "Key steps:\n",
        "1. import neccessry libraries\n",
        "2. split the data\n",
        "3. create the KNN model\n",
        "4. fit the model\n",
        "5. make predictions\n",
        "6. display the confusion matrix\n",
        "7. compute performance metrics\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7-pVQB7IB1jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.1 Import Libraries**\n",
        "importing necessary libraries for data handling, model training and evaluation\n",
        "\n",
        "- `from sklearn.model_selection import train_test_split`\n",
        "\n",
        "  Purpose: This function is used to split a dataset into two subsets: one for training the model and another for testing it.\n",
        "- `from sklearn.neighbors import KNeighborsClassifier`\n",
        "\n",
        "  Purpose: This imports the K-Nearest Neighbors (KNN) classifier (a simple, instance-based learning algorithm used for classification and regression tasks).\n",
        "- `from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, ConfusionMatrixDisplay`\n",
        "\n",
        "  Purpose: These functions are used to evaluate the performance of the machine learning model.\n",
        "- `import seaborn as sns`\n",
        "  \n",
        "  Purpose: Seaborn is a statistical data visualization library built on top of Matplotlib. It'll provide a high-level interface for drawing attractive and informative statistical graphics.\n",
        "- `import matplotlib.pyplot as plt`\n",
        "\n",
        "  Purpose: Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension, NumPy."
      ],
      "metadata": {
        "id": "rsW7fZbYHiZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "id": "tguquFnm6zFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2 Split the data**\n",
        "\n",
        "Here we will be splitting the data into features (X) and target labels (y), and then further splitting those into training and testing sets, we'll ensure that the model can learn from one portion of the data while being evaluated on a separate, unseen portion.\n",
        "\n",
        "My expected outcome:\n",
        "- **X_train**: Contains 80% of the feature data for training the model.\n",
        "- **X_test**: Contains 20% of the feature data for testing the model.\n",
        "- **y_train**: Contains 80% of the target labels corresponding to X_train.\n",
        "- **y_test**: Contains 20% of the target labels corresponding to X_test."
      ],
      "metadata": {
        "id": "oxvldGc1MX5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features\n",
        "X = iris_df.drop('target', axis=1)\n",
        "\n",
        "# Target variable\n",
        "y = iris_df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "8KFy932EKWzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3 Train the KNN Model**\n",
        "\n",
        "I will first carry out cross-validation technique to validate which is the best number of neighbors for my data."
      ],
      "metadata": {
        "id": "ufTH6nM8Uw5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Range of k values to test\n",
        "k_values = range(1, 21)\n",
        "scores = []\n",
        "\n",
        "# Perform cross-validation for each k\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    cv_scores = cross_val_score(knn, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
        "    scores.append(np.mean(cv_scores))\n",
        "\n",
        "# Find the best k\n",
        "best_k = k_values[np.argmax(scores)]\n",
        "print(f'The best number of neighbors is: {best_k}')\n",
        "\n",
        "# Plotting the results\n",
        "plt.plot(k_values, scores)\n",
        "plt.xlabel('Number of Neighbors K')\n",
        "plt.ylabel('Cross-Validated Accuracy')\n",
        "plt.title('KNN Hyperparameter Tuning')\n",
        "plt.xticks(k_values)  # Show all k values on the x-axis\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1sL4gJluXTAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will then initialize the **classifier**\n",
        "\n",
        "This parameter specifies the number of nearest neighbors to consider when making predictions. In this case, the model will look at the 3 closest data points in the training set to determine the class of a new data point."
      ],
      "metadata": {
        "id": "ujRW8I2HYmOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the KNN Model\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "MmfrhpwKSbmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.4 Making Predictions**"
      ],
      "metadata": {
        "id": "pPCoZaG6ZdzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = knn.predict(X_test)"
      ],
      "metadata": {
        "id": "3_3W2dtkZNoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Displaying the Confusion Matrix**\n",
        "\n",
        "The confusion matrix typically has four components:\n",
        "\n",
        "- **True Positives (TP)**: The number of instances correctly predicted as positive.\n",
        "- **True Negatives (TN)**: The number of instances correctly predicted as negative.\n",
        "- **False Positives (FP)**: The number of instances incorrectly predicted as positive (also known as Type I error).\n",
        "- **False Negatives (FN)**: The number of instances incorrectly predicted as negative (also known as Type II error).\n",
        "\n",
        "We should enter a code snippet that computes the confusion matrix for the KNN model's predictions, creates a visual representation of it, and displays the results.\n",
        "\n",
        "This visualization will help in understanding how many instances were correctly or incorrectly classified for each class, providing valuable insights into the model's strengths and weaknesses."
      ],
      "metadata": {
        "id": "wCXi8sO7ajzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names).plot(cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RVAT-AOmaf5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.5 Compute Performance Metrics**\n",
        "\n",
        "I will first calculate the accuracy of the KNN model, usually in theory it's\n",
        "\n",
        "$\\frac{TP + TN}{Total}$\n",
        "\n",
        "Then I will generate a detailed classification report that provides additional performance metrics\n",
        "\n",
        "Then finally, I will print out the accuracy mmodel and the detailed classification report"
      ],
      "metadata": {
        "id": "9rqOgvdsbDE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1-Score: {f1:.4f}')\n",
        "print('Classification Report:\\n', report)"
      ],
      "metadata": {
        "id": "Mxe9YeEGaxAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "qQlNAbd-z2QI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Naive Bayes**\n",
        "**Definition:** This is a family of probabilistic algorithms based on Bayes' theorem, used primarily for classification tasks. It is called \"naive\" because it makes a simplifying assumption that the features (or predictors) are conditionally independent given the class label. This means that the presence of one feature does not affect the presence of another feature within the same class.\n",
        "\n",
        "Steps for Naive Bayes Classification\n",
        "1. **Import Libraries:** Import the necessary libraries for data manipulation, modeling, and evaluation.\n",
        "\n",
        "2. **Load the Dataset:** I already loaded it into a data frame\n",
        "\n",
        "3. **Split the Dataset:** I already split the data into training and testing sets\n",
        "\n",
        "4. **Train the Model:** Instantiate the Naive Bayes classifier and fit it to the training data.\n",
        "\n",
        "5. **Make Predictions:** Use the trained model to make predictions on the test set.\n",
        "\n",
        "6. **Display the Confusion Matrix:** Generate and visualize the confusion matrix to evaluate the model's predictions.\n",
        "\n",
        "7. **Compute Performance Metrics:** Calculate accuracy, precision, recall, F1-score, and any other relevant metrics."
      ],
      "metadata": {
        "id": "WV8QHUr3ZdZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1 Import Libraries**\n",
        "\n",
        "The only library we need to add is *GaussianNB*:\n",
        "\n",
        "`GaussianNB` is an implementation of the Gaussian Naive Bayes algorithm, which is a probabilistic classifier based on Bayes' theorem. It assumes that the features follow a normal (Gaussian) distribution."
      ],
      "metadata": {
        "id": "jNDLDpUdry0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# additional import of the Gaussian Naive Bayes classifier library\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "metadata": {
        "id": "HS7oz_DAbVng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since steps `2.2` & `2.3` have been completed in the KNN classification we move on to `2.4`\n",
        "\n",
        "#### **2.4 Training the model**\n",
        "- I will initialize a new model object `nb_model` that will hold the parameters and methods necessary for training and making predictions with the Naive Bayes algorithm\n",
        "\n",
        "- Then we will train the Naive Bayes model using the training dataset"
      ],
      "metadata": {
        "id": "IAG5itaaszIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_model = GaussianNB()  # Instantiate the Naive Bayes classifier\n",
        "nb_model.fit(X_train, y_train)  # Fit the model to the training data"
      ],
      "metadata": {
        "id": "IBXoNEgjsS1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.5 Making Predictions**\n",
        "\n",
        "We will use the trained Naive Bayes model to make predictions on the test dataset\n",
        "\n",
        "We can't skip this step because the previos `y_pred` variable stored the predictions for the KNN classification, hence we need to regenerate the predictions accordingly."
      ],
      "metadata": {
        "id": "i8DCxJn-vfJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = nb_model.predict(X_test)  # Predict on the test set"
      ],
      "metadata": {
        "id": "4kSX4oaDuhOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.6 Displaying the Confusion Matrix**"
      ],
      "metadata": {
        "id": "UQk6HkcrwOX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names).plot(cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v4HXhc0wIwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.7 Computing Performance Metrics**"
      ],
      "metadata": {
        "id": "KdsFIpy0xbG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Performance Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred) # Sum of Correct Predictions / Total Number of Predictions\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names) # summarizes the metrics for each class\n",
        "specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])  # True Negatives / (True Negatives + False Positives)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print('Classification Report:\\n', report)\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'Specificity: {specificity:.2f}')\n",
        "print(f'F1-Score: {f1:.4f}')"
      ],
      "metadata": {
        "id": "rqEmaCcVxQ8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "O41bhKE1z-li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Support Vector Machine (SVM)**\n",
        "\n",
        "This is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that separates data points of different classes in a high-dimensional space.\n",
        "\n",
        "SVM aims to maximize the margin between the closest points of each class (support vectors) and the hyperplane, which enhances the model's ability to generalize to unseen data. SVM can also use kernel functions to handle non-linear separations by transforming the input space into higher dimensions.\n",
        "\n",
        "Steps to Perform SVM on the Iris Dataset:\n",
        "1. **Import Libraries:** Import the necessary libraries for data manipulation, modeling, and evaluation.\n",
        "\n",
        "2. **Load the Dataset:** I already loaded it into a data frame\n",
        "\n",
        "3. **Split the Dataset:** I already split the data into training and testing sets\n",
        "\n",
        "4. **Train the Model:** Instantiate the Naive Bayes classifier and fit it to the training data.\n",
        "\n",
        "5. **Make Predictions:** Use the trained model to make predictions on the test set.\n",
        "\n",
        "6. **Display the Confusion Matrix:** Generate and visualize the confusion matrix to evaluate the model's predictions.\n",
        "\n",
        "7. **Compute Performance Metrics:** Calculate accuracy, recall, F1-score, and any other relevant metrics."
      ],
      "metadata": {
        "id": "V-kBFIlFzpUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1 Import Libraries**\n",
        "\n",
        "- I will import the SVM library, since the rest have been imported in previous code blocks"
      ],
      "metadata": {
        "id": "zEzo8fVlB2iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "NAEXOZcQxmfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since steps `3.2` & `3.3` have been completed in the KNN classification we move on to `3.4`\n",
        "\n",
        "#### **3.4 Training the SVM model**\n",
        "- I will create an instance of the Support Vector Classifier (SVC) from the sklearn.svm module.\n",
        "- Then I will train the SVM model using the training dataset"
      ],
      "metadata": {
        "id": "LLO2UiGCE3m6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But first let's run k-cross cross-validation to determine which is the best kernel to use for the model (linear, polynomiol, radial basis function, or even sigmoid) based on performance.\n",
        "\n",
        "Kernel Functions:\n",
        "- **Linear Kernel:** This is used when the data is linearly separable in the feature space. It creates a hyperplane that separates the classes with a straight line (or flat hyperplane in higher dimensions).\n",
        "\n",
        "- **Radial Basis Function (RBF) Kernel:** This is a popular choice for non-linear data. It maps input features into an infinite-dimensional space, allowing complex boundaries between classes.\n",
        "\n",
        "- **Polynomial Kernel:** This kernel allows for polynomial decision boundaries and can model interactions between features.\n",
        "\n",
        "- **Other kernels** can also be specified, such as 'sigmoid' and custom kernels, depending on the problem's requirements."
      ],
      "metadata": {
        "id": "GvdSPFGGHFjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I had already imported cross_val_score in a previos code block\n",
        "\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "for kernel in kernels:\n",
        "    svm_model = SVC(kernel=kernel)\n",
        "    scores = cross_val_score(svm_model, X, y, cv=5)  # 5-fold cross-validation\n",
        "    print(f'Kernel: {kernel}, Accuracy: {scores.mean():.2f} ± {scores.std():.2f}')\n"
      ],
      "metadata": {
        "id": "GAob4IPAEvBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will go with the linear kernel"
      ],
      "metadata": {
        "id": "FGOPh44cIrZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "edmnO-GRIiMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.5 Make Predictions**"
      ],
      "metadata": {
        "id": "FjzOvi7eI5ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svm_model.predict(X_test)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "-SXl_ZsjI06G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.6 Displaying the confusion matrix**"
      ],
      "metadata": {
        "id": "Wq4Go7xDJGf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print('Confusion Matrix:\\n', cm)"
      ],
      "metadata": {
        "id": "k5MJGoVwJB8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names).plot(cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VnRWBDg-Jhu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.7 Evaluating Performace Metrics**"
      ],
      "metadata": {
        "id": "ucQ-xTzQJ2mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print('Classification Report:\\n', report)\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'Specificity: {specificity:.2f}')\n",
        "print(f'F1-Score: {f1:.4f}')"
      ],
      "metadata": {
        "id": "SK-EWdjmJypt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Decision Tree**\n",
        "A Decision Tree classifies by recursively splitting data into subsets based on the most significant feature.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "- At each node, select the feature that best separates classes (using Gini impurity or Information Gain/Entropy).\n",
        "\n",
        "- Split data into branches.\n",
        "\n",
        "- Repeat until pure nodes or stopping criteria (max depth, min samples) are met.\n",
        "\n",
        "- Classify new data by traversing the tree to a leaf and outputting the majority class.\n",
        "\n",
        "**Steps to perform Decision Tree Classification on the Iris Dataset:**\n",
        "1. Importing Libraries\n",
        "2. Creating and Training the Decision Tree model\n",
        "3. Making Predictions\n",
        "4. Displaying the Confusion Matrix\n",
        "5. Evaluating the model's metrics\n",
        "6. Visualizing the Decision Tree"
      ],
      "metadata": {
        "id": "qw0eIvosKzOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.1 Importing Libraries**\n",
        "\n",
        "- `DecisionTreeClassifier`\n",
        "- `plot_tree`"
      ],
      "metadata": {
        "id": "0wM7QDnWzlCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier # creates & trains model\n",
        "from sklearn.tree import plot_tree # visualizes the trained tree structure"
      ],
      "metadata": {
        "id": "3JShEJA-KSna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.2 Creating and Training the Decision Tree mode**"
      ],
      "metadata": {
        "id": "XAmoQcfx0R0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first determine the best parameters to use using `GridSearchCV`"
      ],
      "metadata": {
        "id": "LH3ztfCd05sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(grid_search.best_params_)\n",
        "best_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "-RMTH-3l0RJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_dt_model = DecisionTreeClassifier(\n",
        "    criterion='entropy',\n",
        "    max_depth=5,\n",
        "    min_samples_leaf=4,\n",
        "    min_samples_split=2,\n",
        "    random_state=42\n",
        ")\n",
        "best_dt_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "cOOJ96Pn1GPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.3 Make Predictions**"
      ],
      "metadata": {
        "id": "UiVHNB3L2M3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_dt_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "fytcngGn2J27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.4 Display confusion Matrix**"
      ],
      "metadata": {
        "id": "Pvj96eVR2ZjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print('Confusion Matrix:\\n', cm)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names).plot(cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BLIY15lY2XTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.5 Evaluationg the model's metrics**"
      ],
      "metadata": {
        "id": "a-s7Yf-i20pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print('Classification Report:\\n', report)\n",
        "print(f'Specificity: {specificity:.2f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1-Score: {f1:.4f}')"
      ],
      "metadata": {
        "id": "zygEO-D62t2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.6 Visualize the Decision Tree**"
      ],
      "metadata": {
        "id": "APGU5_k73IjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plot_tree(best_dt_model, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
        "plt.title('Decision Tree Visualization')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5G4tS18J3GHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Random Forest Classification**\n",
        "\n",
        "This is an ensemble method combining multiple decision trees:\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "- Creates many trees on bootstrap samples (random sampling with replacement)\n",
        "\n",
        "- Each split considers random subset of features (feature bagging)\n",
        "\n",
        "- Final prediction = majority vote (classification) or average (regression)\n",
        "\n",
        "**Steps to implment Random Forest**:\n",
        "1. Import Libraries\n",
        "2. Train the Random Forest Model\n",
        "3. Make Predictions\n",
        "4. Display confusion matrix\n",
        "5. Compute Metrics\n"
      ],
      "metadata": {
        "id": "6y0aYji17A1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5.1 Import Libreries**"
      ],
      "metadata": {
        "id": "6qNWxhhj91KO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "MzoGRF9S3UDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5.2 Train Random Forest Model**"
      ],
      "metadata": {
        "id": "apZ6Ae9i_d3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "gLsB7Mkd_c1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5.3 Make Predictions**"
      ],
      "metadata": {
        "id": "pjh9lf4f_tfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = rf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "4cR0lvXl_oTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5.4 Display Confusion matrix**"
      ],
      "metadata": {
        "id": "UZ2awFPA_z79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=iris.target_names,\n",
        "            yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SaQ5otYP_zDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5.5 Evaluate Metrics**"
      ],
      "metadata": {
        "id": "bvRQX5tkALtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1-Score: {f1:.4f}')"
      ],
      "metadata": {
        "id": "sO8QSMKWAF3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PB7-WuQIATmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Regression Algorithms**"
      ],
      "metadata": {
        "id": "KfBrpTanLyR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1**\n",
        "\n",
        "I will import the California dataset and use it to implement the algorithms below\n",
        "\n",
        "> The official scikit-learn California Housing dataset contains **20,640 samples** with **8 numeric features** and **1 target** variable, **no missing values**\n",
        "\n",
        "**Features (all per block group)** :\n",
        "- `MedInc`: Median income\n",
        "- `HouseAge`: Median house age  \n",
        "- `AveRooms`: Average rooms per household\n",
        "- `AveBedrms`: Average bedrooms per household\n",
        "- `Population`: Block group population\n",
        "- `AveOccup`: Average household members\n",
        "- `Latitude` / `Longitude`: Geographic coordinates\n",
        "\n",
        "**Target**: `MedHouseVal` - Median house value (**$100,000 units**)\n",
        "\n",
        "Derived from **1990 U.S. census**; regression task only (not classification) ."
      ],
      "metadata": {
        "id": "xzGf7W1DP5Tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading & making a dataframe for the California Housing Dataset**\n",
        "\n",
        "- I will import the official scikit-learn California dataset together with libraries that will be used in this section\n",
        "- I will create a dataframe for the dataset with target values\n",
        "- I will display the first few rows"
      ],
      "metadata": {
        "id": "HOXmlAhzQyUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing # importing the dataset\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score # for metrics evaluation"
      ],
      "metadata": {
        "id": "2cIqUBh7MKjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "# Create dataframe\n",
        "housing_df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "housing_df['target'] = housing.target  # Add target column\n",
        "\n",
        "print(housing_df.head())"
      ],
      "metadata": {
        "id": "bCTJ5Cv3C-wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2**\n",
        "\n",
        "We will carry out the following regression systems:\n",
        "- Linear Regression\n",
        "\n",
        "- Polynomial Regression\n",
        "\n",
        "- Lasso Regression\n",
        "\n",
        "- Ridge Regression\n",
        "\n",
        "Ensuring the following steps are carried out fpr each model:\n",
        "- Train the model\n",
        "\n",
        "- Make predictions\n",
        "\n",
        "- Evaluate using MAE, MSE, RMSE, and R²\n",
        "\n"
      ],
      "metadata": {
        "id": "1kCVEEblXc2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Linear Regression**\n",
        "\n",
        "**Definition**\n",
        "So based on what I've found out, this algorithm models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. It assumes the relationship is approximately linear and finds the optimal coefficients (weights) that minimize the sum of squared residuals between actual and predicted values. For a single feature, it's a line ($y = mx + b$); for multiple features, it's a hyperplane ($y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$). It's simple, interpretable, and serves as a baseline for regression tasks, but assumes linearity, independence, homoscedasticity, and normality of errors.\n",
        "\n",
        "**Steps to implement Linear Regression:**\n",
        "1. Import necessary libraries & prepare Features and Target\n",
        "2. Split data\n",
        "3. Train Linear Regression Model\n",
        "4. Make Predictions\n",
        "5. Evaluate the model\n",
        "6. Cross-validation\n",
        "7. View Coefficients\n",
        "8. Show first few predictions"
      ],
      "metadata": {
        "id": "GyDVLdcEY40w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.1 Prepare features and target**\n",
        "\n",
        "Separates features (predictors) from target (what we're predicting).\n",
        "\n",
        "- `X`: Feature matrix - all columns used to make predictions (MedInc, HouseAge, etc.)\n",
        "\n",
        "- `y`: Target vector - the value we want to predict (median house value)"
      ],
      "metadata": {
        "id": "OnP4oUtRfrDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Prepare features and target & import libraries\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = housing_df.drop('target', axis=1)\n",
        "y = housing_df['target']"
      ],
      "metadata": {
        "id": "V-tPh0gcVcA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2 Split the data**"
      ],
      "metadata": {
        "id": "x1gzLLnCgH2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "i_39qCH8gD6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3 Train the Linear Regression Model**"
      ],
      "metadata": {
        "id": "yTRBUbyKgRpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train Linear Regression model\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "LN_YJX0-gPZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.4 Make predictions**"
      ],
      "metadata": {
        "id": "EIeou5EZjS3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lr.predict(X_test)"
      ],
      "metadata": {
        "id": "S5-wPnFNgdl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.5 Evaluate model**"
      ],
      "metadata": {
        "id": "WNAHAwrXjZ_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'RMSE: ${rmse*100000:.2f}')\n",
        "print(f'MAE: ${mae*100000:.2f}')\n",
        "print(f'R² Score: {r2:.4f}')"
      ],
      "metadata": {
        "id": "OY8bn620jZjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.6 Cross-validation**\n",
        "\n",
        "Cross-validation is necessary because:\n",
        "\n",
        "1. **More reliable estimate:** The single train/test split R² might be lucky or unlucky. CV averages performance across 5 different splits.\n",
        "\n",
        "2. **Detects overfitting:** If CV score is much lower than test score, your model is overfitting.\n",
        "\n",
        "3. **Better use of data:** Every sample gets used for both training and validation.\n",
        "\n",
        "4. **Stability check:** The standard deviation tells you if performance varies wildly across different data subsets."
      ],
      "metadata": {
        "id": "xUAutQwbkTvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_scores = cross_val_score(lr, X, y, cv=5, scoring='r2')\n",
        "print(f'Cross-val R²: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})')"
      ],
      "metadata": {
        "id": "nqYITN-xjgGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means:\n",
        "\n",
        "- **Average R²:** ~55.3% of variance in house prices is explained by the model across the 5 folds\n",
        "\n",
        "- **Stability:** ±0.0617 standard deviation - performance varies by about 6 percentage points depending on which data subset is used for validation"
      ],
      "metadata": {
        "id": "0yBzPRSelqpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.7 View Coeficients**\n",
        "\n",
        "Shows which features most influence predictions and how they affect house values."
      ],
      "metadata": {
        "id": "MTZyzZFXlxhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coef_df = pd.DataFrame({'Feature': housing.feature_names, 'Coefficient': lr.coef_})\n",
        "print(coef_df.sort_values('Coefficient', key=abs, ascending=False))"
      ],
      "metadata": {
        "id": "NKlkcjzZlR8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.8 Show first few predictions vs actual**"
      ],
      "metadata": {
        "id": "lfw3tXgfmONg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Show first few predictions vs actual\n",
        "results_df = pd.DataFrame({'Actual': y_test[:5], 'Predicted': y_pred[:5]})\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "UVjM4m3Pl9Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite potentially decent aggregate metrics, model fails on specific cases\n",
        "\n",
        "Based on these results, I think Linear regression is too rigid for this data, maybe I have possible missing features, or I should try `log(price)` instead"
      ],
      "metadata": {
        "id": "l8SABPlom0be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Polynomial Regression Algorithm**\n",
        "\n",
        "**Polynomial Regression** is a form of regression analysis that models the relationship between variables as an\n",
        "-th degree polynomial."
      ],
      "metadata": {
        "id": "D36QbxrVKEjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps to implement Polynomial Regression**:\n",
        "1. Import Libraries\n",
        "-  Load the Carlifornia Dataset (Already achieved in the previous algorithm)\n",
        "-  Split the Dataset (Already achieved in the previous algorithm)\n",
        "2. Create Polynomial Features\n",
        "3. Train the Model\n",
        "4. Make predictions\n",
        "5. Evaluate the model"
      ],
      "metadata": {
        "id": "rg3DsitP1nFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1 Import Libraries**\n",
        "\n",
        "- import `PolynomialFeatures` class, that is essential for transforming input features into polynomial features, enabling polynomial regression"
      ],
      "metadata": {
        "id": "r2pAXpNE2efc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures"
      ],
      "metadata": {
        "id": "aUIIii6QmYm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2 Create Polynomial Features**"
      ],
      "metadata": {
        "id": "MfX-wnP53PUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degree = 2\n",
        "poly_features = PolynomialFeatures(degree=degree)\n",
        "\n",
        "x_train_poly = poly_features.fit_transform(X_train)\n",
        "x_test_poly = poly_features.transform"
      ],
      "metadata": {
        "id": "iTLLHiOV4BD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Train The Model**\n",
        "Creating a lear regression model to fit it to the trasformed training data"
      ],
      "metadata": {
        "id": "FYoBQx5c4PiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(x_train_poly, y_train)"
      ],
      "metadata": {
        "id": "yLucxjaN4Ayk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Make Predictions**\n",
        "I will use the trained model to make predicions on the test set"
      ],
      "metadata": {
        "id": "63xd7vI94hTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test_poly)"
      ],
      "metadata": {
        "id": "5Eb6DJ7W4dIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Evaluate the Model**\n",
        "I will use Mean Absolute Error(MAE), Mean Squared Error(MSE), Root Mean Squared Error(RMSE), and $R^2$ score"
      ],
      "metadata": {
        "id": "-RTr82Zt4vf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'MAE: {mae:.2f}')\n",
        "print(f'MSE: {mse:.2f}')\n",
        "print(f'RMSE: {rmse:.2f}')\n",
        "print(f'R²: {r2:.2f}')\n"
      ],
      "metadata": {
        "id": "eNT3YPOL6SPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Lasso Regression**\n",
        "**Lasso Regression** (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that incorporates L1 regularization. It adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function, which helps prevent overfitting and can lead to sparse models by driving some coefficients to zero. This makes Lasso useful for feature selection in high-dimensional datasets.\n",
        "\n",
        "**Steps to implement the Lasoo Regression:**\n",
        "1. Import Libraries\n",
        "2. Train the model\n",
        "3. Make Predictions\n",
        "4. Evaluate the model"
      ],
      "metadata": {
        "id": "KWdqHnd9D1Km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1 Import Libraries**\n",
        "\n",
        "- import `Lasso`: This class is used to implement Lasso Regression, which applies L1 regularization to linear regression models, helping to prevent overfitting and enabling feature selection by shrinking some coefficients to zero."
      ],
      "metadata": {
        "id": "PEUBEIIgGEZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso"
      ],
      "metadata": {
        "id": "725H23alGD9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.2 Train the Model**\n",
        "Create a Lasso regression model and fit it to the training data."
      ],
      "metadata": {
        "id": "oXQuSzFuGcrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_model = Lasso(alpha=1.0)\n",
        "lasso_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "PLoNYIRaGipw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.3 Make Predictions**\n",
        "Use the trained Lasso model to make predictions on the test set."
      ],
      "metadata": {
        "id": "W6hI8euuGb99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lasso_model.predict(X_test)"
      ],
      "metadata": {
        "id": "txuVujh_G2D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.4 Evaluate the Model**\n",
        "Calculate evaluation metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R² score"
      ],
      "metadata": {
        "id": "UbC_dUhLGbEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'MAE: {mae:.2f}')\n",
        "print(f'MSE: {mse:.2f}')\n",
        "print(f'RMSE: {rmse:.2f}')\n",
        "print(f'R²: {r2:.2f}')"
      ],
      "metadata": {
        "id": "kT9oqPhSHIU2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}